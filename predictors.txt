predictors (5 anos de histórico, XAUUSD)

M5_OHLC, csv_o_XAUUSD_2018-01-02_2023-09-15
M5_OHLCV, csv_o_XAUUSD_2018-01-02_2023-09-15
M10_OHLC, csv_o_XAUUSD_2018-01-02_2023-09-15
M10_OHLCV, csv_o_XAUUSD_2018-01-02_2023-09-15
M15_OHLC, csv_o_XAUUSD_2018-01-02_2023-09-15
M15_OHLCV, csv_o_XAUUSD_2018-01-02_2023-09-15
M20_OHLC, csv_o_XAUUSD_2018-01-02_2023-09-15
M20_OHLCV, csv_o_XAUUSD_2018-01-02_2023-09-15
M30_OHLC, csv_o_XAUUSD_2018-01-02_2023-09-15
M30_OHLCV, csv_o_XAUUSD_2018-01-02_2023-09-15
H1_OHLC, csv_o_XAUUSD_2018-01-02_2023-09-15
H1_OHLCV, csv_o_XAUUSD_2018-01-02_2023-09-15

------

M5_OHLC:
M5_OHLC_S2_HL1 -> OHLC, n_steps = 2, n_hidden_layers = 1 ok
M5_OHLC_S2_HL2 -> OHLC, n_steps = 2, n_hidden_layers = 2
M5_OHLC_S2_HL3 -> OHLC, n_steps = 2, n_hidden_layers = 3

M5_OHLC_S4_HL1 -> OHLC, n_steps = 4, n_hidden_layers = 1 ok
M5_OHLC_S4_HL2 -> OHLC, n_steps = 4, n_hidden_layers = 2
M5_OHLC_S4_HL3 -> OHLC, n_steps = 4, n_hidden_layers = 3

M5_OHLC_S6_HL1 -> OHLC, n_steps = 6, n_hidden_layers = 1 ok
M5_OHLC_S6_HL2 -> OHLC, n_steps = 6, n_hidden_layers = 2
M5_OHLC_S6_HL3 -> OHLC, n_steps = 6, n_hidden_layers = 3

------

M5_OHLCV:
M5_OHLCV_S2_HL1 -> OHLCV, n_steps = 2, n_hidden_layers = 1 ok
M5_OHLCV_S2_HL2 -> OHLCV, n_steps = 2, n_hidden_layers = 2
M5_OHLCV_S2_HL3 -> OHLCV, n_steps = 2, n_hidden_layers = 3

M5_OHLCV_S4_HL1 -> OHLCV, n_steps = 4, n_hidden_layers = 1 ok
M5_OHLCV_S4_HL2 -> OHLCV, n_steps = 4, n_hidden_layers = 2
M5_OHLCV_S4_HL3 -> OHLCV, n_steps = 4, n_hidden_layers = 3

M5_OHLCV_S6_HL1 -> OHLCV, n_steps = 6, n_hidden_layers = 1 ok
M5_OHLCV_S6_HL2 -> OHLCV, n_steps = 6, n_hidden_layers = 2
M5_OHLCV_S6_HL3 -> OHLCV, n_steps = 6, n_hidden_layers = 3

------

M10_OHLC:
M10_OHLC_S2_HL1 -> OHLC, n_steps = 2, n_hidden_layers = 1 ok
M10_OHLC_S2_HL2 -> OHLC, n_steps = 2, n_hidden_layers = 2 ok
M10_OHLC_S2_HL3 -> OHLC, n_steps = 2, n_hidden_layers = 3 ok

M10_OHLC_S4_HL1 -> OHLC, n_steps = 4, n_hidden_layers = 1 ok
M10_OHLC_S4_HL2 -> OHLC, n_steps = 4, n_hidden_layers = 2 ok
M10_OHLC_S4_HL3 -> OHLC, n_steps = 4, n_hidden_layers = 3 ok

M10_OHLC_S6_HL1 -> OHLC, n_steps = 6, n_hidden_layers = 1 ok
M10_OHLC_S6_HL2 -> OHLC, n_steps = 6, n_hidden_layers = 2 ok
M10_OHLC_S6_HL3 -> OHLC, n_steps = 6, n_hidden_layers = 3 ok

------

M10_OHLCV:
M10_OHLCV_S2_HL1 -> OHLCV, n_steps = 2, n_hidden_layers = 1 ok
M10_OHLCV_S2_HL2 -> OHLCV, n_steps = 2, n_hidden_layers = 2 ok
M10_OHLCV_S2_HL3 -> OHLCV, n_steps = 2, n_hidden_layers = 3 ok

M10_OHLCV_S4_HL1 -> OHLCV, n_steps = 4, n_hidden_layers = 1 ok
M10_OHLCV_S4_HL2 -> OHLCV, n_steps = 4, n_hidden_layers = 2 ok
M10_OHLCV_S4_HL3 -> OHLCV, n_steps = 4, n_hidden_layers = 3 ok

M10_OHLCV_S6_HL1 -> OHLCV, n_steps = 6, n_hidden_layers = 1 ok
M10_OHLCV_S6_HL2 -> OHLCV, n_steps = 6, n_hidden_layers = 2 ok
M10_OHLCV_S6_HL3 -> OHLCV, n_steps = 6, n_hidden_layers = 3 ok

------

M15_OHLC:
M15_OHLC_S2_HL1 -> OHLC, n_steps = 2, n_hidden_layers = 1 ok
M15_OHLC_S2_HL2 -> OHLC, n_steps = 2, n_hidden_layers = 2
M15_OHLC_S2_HL3 -> OHLC, n_steps = 2, n_hidden_layers = 3

M15_OHLC_S4_HL1 -> OHLC, n_steps = 4, n_hidden_layers = 1 ok
M15_OHLC_S4_HL2 -> OHLC, n_steps = 4, n_hidden_layers = 2
M15_OHLC_S4_HL3 -> OHLC, n_steps = 4, n_hidden_layers = 3

M15_OHLC_S6_HL1 -> OHLC, n_steps = 6, n_hidden_layers = 1 ok
M15_OHLC_S6_HL2 -> OHLC, n_steps = 6, n_hidden_layers = 2
M15_OHLC_S6_HL3 -> OHLC, n_steps = 6, n_hidden_layers = 3

------

M15_OHLCV:
M15_OHLCV_S2_HL1 -> OHLCV, n_steps = 2, n_hidden_layers = 1 ok
M15_OHLCV_S2_HL2 -> OHLCV, n_steps = 2, n_hidden_layers = 2
M15_OHLCV_S2_HL3 -> OHLCV, n_steps = 2, n_hidden_layers = 3

M15_OHLCV_S4_HL1 -> OHLCV, n_steps = 4, n_hidden_layers = 1 ok
M15_OHLCV_S4_HL2 -> OHLCV, n_steps = 4, n_hidden_layers = 2
M15_OHLCV_S4_HL3 -> OHLCV, n_steps = 4, n_hidden_layers = 3

M15_OHLCV_S6_HL1 -> OHLCV, n_steps = 6, n_hidden_layers = 1 ok
M15_OHLCV_S6_HL2 -> OHLCV, n_steps = 6, n_hidden_layers = 2
M15_OHLCV_S6_HL3 -> OHLCV, n_steps = 6, n_hidden_layers = 3

------

M20_OHLC:
M20_OHLC_S2_HL1 -> OHLC, n_steps = 2, n_hidden_layers = 1 ok
M20_OHLC_S2_HL2 -> OHLC, n_steps = 2, n_hidden_layers = 2
M20_OHLC_S2_HL3 -> OHLC, n_steps = 2, n_hidden_layers = 3

M20_OHLC_S4_HL1 -> OHLC, n_steps = 4, n_hidden_layers = 1 ok
M20_OHLC_S4_HL2 -> OHLC, n_steps = 4, n_hidden_layers = 2
M20_OHLC_S4_HL3 -> OHLC, n_steps = 4, n_hidden_layers = 3

M20_OHLC_S6_HL1 -> OHLC, n_steps = 6, n_hidden_layers = 1 ok
M20_OHLC_S6_HL2 -> OHLC, n_steps = 6, n_hidden_layers = 2
M20_OHLC_S6_HL3 -> OHLC, n_steps = 6, n_hidden_layers = 3

------

M20_OHLCV:
M20_OHLCV_S2_HL1 -> OHLCV, n_steps = 2, n_hidden_layers = 1 ok
M20_OHLCV_S2_HL2 -> OHLCV, n_steps = 2, n_hidden_layers = 2
M20_OHLCV_S2_HL3 -> OHLCV, n_steps = 2, n_hidden_layers = 3

M20_OHLCV_S4_HL1 -> OHLCV, n_steps = 4, n_hidden_layers = 1 ok
M20_OHLCV_S4_HL2 -> OHLCV, n_steps = 4, n_hidden_layers = 2
M20_OHLCV_S4_HL3 -> OHLCV, n_steps = 4, n_hidden_layers = 3

M20_OHLCV_S6_HL1 -> OHLCV, n_steps = 6, n_hidden_layers = 1 ok
M20_OHLCV_S6_HL2 -> OHLCV, n_steps = 6, n_hidden_layers = 2
M20_OHLCV_S6_HL3 -> OHLCV, n_steps = 6, n_hidden_layers = 3

------

M30_OHLC:
M30_OHLC_S2_HL1 -> OHLC, n_steps = 2, n_hidden_layers = 1 ---
M30_OHLC_S2_HL2 -> OHLC, n_steps = 2, n_hidden_layers = 2
M30_OHLC_S2_HL3 -> OHLC, n_steps = 2, n_hidden_layers = 3

M30_OHLC_S4_HL1 -> OHLC, n_steps = 4, n_hidden_layers = 1
M30_OHLC_S4_HL2 -> OHLC, n_steps = 4, n_hidden_layers = 2
M30_OHLC_S4_HL3 -> OHLC, n_steps = 4, n_hidden_layers = 3

M30_OHLC_S6_HL1 -> OHLC, n_steps = 6, n_hidden_layers = 1
M30_OHLC_S6_HL2 -> OHLC, n_steps = 6, n_hidden_layers = 2
M30_OHLC_S6_HL3 -> OHLC, n_steps = 6, n_hidden_layers = 3

------

M30_OHLCV:
M30_OHLCV_S2_HL1 -> OHLCV, n_steps = 2, n_hidden_layers = 1
M30_OHLCV_S2_HL2 -> OHLCV, n_steps = 2, n_hidden_layers = 2
M30_OHLCV_S2_HL3 -> OHLCV, n_steps = 2, n_hidden_layers = 3

M30_OHLCV_S4_HL1 -> OHLCV, n_steps = 4, n_hidden_layers = 1
M30_OHLCV_S4_HL2 -> OHLCV, n_steps = 4, n_hidden_layers = 2
M30_OHLCV_S4_HL3 -> OHLCV, n_steps = 4, n_hidden_layers = 3

M30_OHLCV_S6_HL1 -> OHLCV, n_steps = 6, n_hidden_layers = 1
M30_OHLCV_S6_HL2 -> OHLCV, n_steps = 6, n_hidden_layers = 2
M30_OHLCV_S6_HL3 -> OHLCV, n_steps = 6, n_hidden_layers = 3

------

H1_OHLC:
H1_OHLC_S2_HL1 -> OHLC, n_steps = 2, n_hidden_layers = 1 ok
H1_OHLC_S2_HL2 -> OHLC, n_steps = 2, n_hidden_layers = 2
H1_OHLC_S2_HL3 -> OHLC, n_steps = 2, n_hidden_layers = 3

H1_OHLC_S4_HL1 -> OHLC, n_steps = 4, n_hidden_layers = 1 ok
H1_OHLC_S4_HL2 -> OHLC, n_steps = 4, n_hidden_layers = 2
H1_OHLC_S4_HL3 -> OHLC, n_steps = 4, n_hidden_layers = 3

H1_OHLC_S6_HL1 -> OHLC, n_steps = 6, n_hidden_layers = 1 ok
H1_OHLC_S6_HL2 -> OHLC, n_steps = 6, n_hidden_layers = 2
H1_OHLC_S6_HL3 -> OHLC, n_steps = 6, n_hidden_layers = 3

------

H1_OHLCV:
H1_OHLCV_S2_HL1 -> OHLCV, n_steps = 2, n_hidden_layers = 1 ok
H1_OHLCV_S2_HL2 -> OHLCV, n_steps = 2, n_hidden_layers = 2
H1_OHLCV_S2_HL3 -> OHLCV, n_steps = 2, n_hidden_layers = 3

H1_OHLCV_S4_HL1 -> OHLCV, n_steps = 4, n_hidden_layers = 1 ok
H1_OHLCV_S4_HL2 -> OHLCV, n_steps = 4, n_hidden_layers = 2
H1_OHLCV_S4_HL3 -> OHLCV, n_steps = 4, n_hidden_layers = 3

H1_OHLCV_S6_HL1 -> OHLCV, n_steps = 6, n_hidden_layers = 1 ok
H1_OHLCV_S6_HL2 -> OHLCV, n_steps = 6, n_hidden_layers = 2
H1_OHLCV_S6_HL3 -> OHLCV, n_steps = 6, n_hidden_layers = 3

------

M10_OHLC e M10_OHLCV:
    candle_input_type: OHLC, OHLCV
    n_steps = 2, 4, 6
    n_hidden_layers = 1, 2, 3
    n_samples_train = 199000

    validation_split = 0.2
    n_samples_test = 3000
    # horizontally stack columns
    dataset_train = prepare_train_data2(hist, symbol_out, 0, n_samples_train, candle_input_type, candle_output_type)

    # convert into input/output samples
    X_train, y_train = split_sequences2(dataset_train, n_steps, candle_output_type)

    # We are now ready to fit a 1D CNN model on this data, specifying the expected number of time steps and
    # features to expect for each input sample.
    n_features = X_train.shape[2]
    n_inputs = n_steps * n_features

    max_n_epochs = 100, patience = 3 e 9

    n_symbols = len(hist.symbols)

    print(f'symbols = {hist.symbols}')
    print(f'n_symbols = {n_symbols}, n_features (n_cols) = {n_features}, n_steps = {n_steps}, n_inputs = {n_inputs}, '
          f'n_hidden_layers = {n_hidden_layers},\ntipo_vela_entrada = {candle_input_type}, '
          f'tipo_vela_saída = {candle_output_type}, n_samples_train = {n_samples_train}, '
          f'\nvalidation_split = {validation_split}, max_n_epochs = {max_n_epochs}, patience = {patience}')

    model = Sequential()
    n_filters = n_features
    kernel_size = n_steps
    pool_size = n_inputs
    n_neurons = n_inputs

    # define cnn model
    # input layer
    model.add(Conv1D(filters=n_filters, kernel_size=kernel_size, activation='relu', input_shape=(n_steps, n_features)))
    model.add(MaxPooling1D(pool_size=pool_size, padding='same'))
    model.add(Flatten())

    # hidden layers
    for i in range(n_hidden_layers):
        model.add(Dense(n_neurons, activation='relu'))

    # define MLP model
    # n_input = X_train.shape[1] * X_train.shape[2]
    # X_train = X_train.reshape((X_train.shape[0], n_input))
    # model.add(Dense(n_inputs, activation='relu', input_dim=n_input))
    # model.add(Dense(n_inputs, activation='relu'))

    # output layer
    model.add(Dense(len(candle_output_type)))
    model.compile(optimizer='adam', loss='mse')
    model_config = model.get_config()

    # fit model
    print(f'treinando o modelo em parte das amostras de treinamento.')
    print(f'n_samples_train * validation_split = {n_samples_train} * {validation_split} = '
          f'{int(n_samples_train * validation_split)}).')

    callbacks = [EarlyStopping(monitor='val_loss', patience=patience, verbose=1),
                 ModelCheckpoint(filepath='model.h5', monitor='val_loss', save_best_only=True, verbose=1)]
    history = model.fit(X_train, y_train, epochs=max_n_epochs, verbose=1,
                        validation_split=validation_split, callbacks=callbacks)

    effective_n_epochs = len(history.history['loss'])
    loss, val_loss = history.history['loss'], history.history['val_loss']
    i_min_loss, i_min_val_loss = np.argmin(loss), np.argmin(val_loss)
    min_loss, min_val_loss = loss[i_min_loss], val_loss[i_min_val_loss]
    losses = {'min_loss': {'value': min_loss, 'index': i_min_loss, 'epoch': i_min_loss + 1},
              'min_val_loss': {'value': min_val_loss, 'index': i_min_val_loss, 'epoch': i_min_val_loss + 1}}

    print(f'avaliando o modelo no conjunto inteiro das amostras de treinamento.')
    saved_model = load_model('model.h5')
    whole_set_train_loss_eval = saved_model.evaluate(X_train, y_train, verbose=0)
    print(f'whole_set_train_loss_eval: {whole_set_train_loss_eval:} (n_samples_train = {n_samples_train})')

    print(f'avaliando o modelo num novo conjunto de amostras de teste.')
    samples_index_start = n_samples_train
    dataset_test = prepare_train_data2(hist, symbol_out, samples_index_start, n_samples_test, candle_input_type,
                                       candle_output_type)

    X_test, y_test = split_sequences2(dataset_test, n_steps, candle_output_type)

    # for MLP model only
    # n_input = X_test.shape[1] * X_test.shape[2]
    # X_test = X_test.reshape((X_test.shape[0], n_input))

    saved_model = load_model('model.h5')
    test_loss_eval = saved_model.evaluate(X_test, y_test, verbose=0)
    print(f'test_loss_eval: {test_loss_eval} (n_samples_test = {n_samples_test})')

    train_config = {'symbol_out': symbol_out,
                    'timeframe': hist.timeframe,
                    'n_steps': n_steps,
                    'candle_input_type': candle_input_type,
                    'candle_output_type': candle_output_type,
                    'n_symbols': n_symbols,
                    'n_features': n_features,
                    'n_inputs': n_inputs,
                    'n_hidden_layers': n_hidden_layers,
                    'n_samples_train': n_samples_train,
                    'validation_split': validation_split,
                    'effective_n_epochs': effective_n_epochs,
                    'max_n_epochs': max_n_epochs,
                    'patience': patience,
                    'whole_set_train_loss_eval': whole_set_train_loss_eval,
                    'n_samples_test': n_samples_test,
                    'test_loss_eval': test_loss_eval,
                    'losses': losses,
                    'symbols': hist.symbols,
                    'bias': 0.0,
                    'n_samples_test_for_calc_bias': 0,
                    'model_config': model_config,
                    'history': history.history}

    return train_config

------

demais predictors:
    candle_input_type: OHLC, OHLCV
    n_steps = 2, 4, 6
    n_hidden_layers = 1, 2, 3

    samples_test_ratio = 0.02

    n_rows = hist.arr[symbol_out][timeframe].shape[0]

    # Número de amostras inéditas usadas na fase de avaliação.
    n_samples_test = int(n_rows * samples_test_ratio)

    n_samples_train = n_rows - n_samples_test  # Número de amostras usadas na fase de treinamento e validação
    validation_split = 0.2

    # horizontally stack columns
    dataset_train = prepare_train_data2(hist, symbol_out, 0, n_samples_train, candle_input_type, candle_output_type)

    # convert into input/output samples
    X_train, y_train = split_sequences2(dataset_train, n_steps, candle_output_type)

    # We are now ready to fit a 1D CNN model on this data, specifying the expected number of time steps and
    # features to expect for each input sample.
    n_features = X_train.shape[2]
    n_inputs = n_steps * n_features

    max_n_epochs = 150, patience = 5 e 15

    n_symbols = len(hist.symbols)

    print(f'n_symbols = {n_symbols}')
    print(f'symbols = {hist.symbols}')
    print(f'tipo_vela_entrada = {candle_input_type}, n_steps = {n_steps}, n_hidden_layers = {n_hidden_layers}\n'
          f'tipo_vela_saída = {candle_output_type}, max_n_epochs = {max_n_epochs}, patience = {patience}\n'
          f'n_features (n_cols) = {n_features}, n_inputs = {n_inputs}\n'
          f'validation_split = {validation_split}, samples_test_ratio = {samples_test_ratio}\n'
          f'n_samples_train = {n_samples_train}, n_samples_test = {n_samples_test}')

    model = Sequential()
    n_filters = n_features
    kernel_size = n_steps
    pool_size = n_inputs
    n_neurons = n_inputs

    # define cnn model
    # input layer
    model.add(Conv1D(filters=n_filters, kernel_size=kernel_size, activation='relu', input_shape=(n_steps, n_features)))
    model.add(MaxPooling1D(pool_size=pool_size, padding='same'))
    model.add(Flatten())

    # hidden layers
    for i in range(n_hidden_layers):
        model.add(Dense(n_neurons, activation='relu'))

    # define MLP model
    # n_input = X_train.shape[1] * X_train.shape[2]
    # X_train = X_train.reshape((X_train.shape[0], n_input))
    # model.add(Dense(n_inputs, activation='relu', input_dim=n_input))
    # model.add(Dense(n_inputs, activation='relu'))

    # output layer
    model.add(Dense(len(candle_output_type)))
    model.compile(optimizer='adam', loss='mse')
    model_config = model.get_config()

    # fit model
    print(f'treinando o modelo em parte das amostras de treinamento.')
    print(f'n_samples_train * validation_split = {n_samples_train} * {validation_split} = '
          f'{int(n_samples_train * validation_split)}).')

    callbacks = [EarlyStopping(monitor='val_loss', patience=patience, verbose=1),
                 ModelCheckpoint(filepath='model.h5', monitor='val_loss', save_best_only=True, verbose=1)]
    history = model.fit(X_train, y_train, epochs=max_n_epochs, verbose=1,
                        validation_split=validation_split, callbacks=callbacks)

    effective_n_epochs = len(history.history['loss'])
    loss, val_loss = history.history['loss'], history.history['val_loss']
    i_min_loss, i_min_val_loss = np.argmin(loss), np.argmin(val_loss)
    min_loss, min_val_loss = loss[i_min_loss], val_loss[i_min_val_loss]
    losses = {'min_loss': {'value': min_loss, 'index': i_min_loss, 'epoch': i_min_loss + 1},
              'min_val_loss': {'value': min_val_loss, 'index': i_min_val_loss, 'epoch': i_min_val_loss + 1}}

    print(f'avaliando o modelo no conjunto inteiro das amostras de treinamento.')
    saved_model = load_model('model.h5')
    whole_set_train_loss_eval = saved_model.evaluate(X_train, y_train, verbose=0)
    print(f'whole_set_train_loss_eval: {whole_set_train_loss_eval:} (n_samples_train = {n_samples_train})')

    print(f'avaliando o modelo num novo conjunto de amostras de teste.')
    samples_index_start = n_samples_train - 1
    dataset_test = prepare_train_data2(hist, symbol_out, samples_index_start, n_samples_test, candle_input_type,
                                       candle_output_type)

    X_test, y_test = split_sequences2(dataset_test, n_steps, candle_output_type)

    # for MLP model only
    # n_input = X_test.shape[1] * X_test.shape[2]
    # X_test = X_test.reshape((X_test.shape[0], n_input))

    saved_model = load_model('model.h5')
    test_loss_eval = saved_model.evaluate(X_test, y_test, verbose=0)
    print(f'test_loss_eval: {test_loss_eval} (n_samples_test = {n_samples_test})')

    product = whole_set_train_loss_eval * test_loss_eval
    print(f'p_{random_seed} = {whole_set_train_loss_eval} * {test_loss_eval} = {product} patience={patience}')

    train_config = {'symbol_out': symbol_out,
                    'timeframe': hist.timeframe,
                    'n_steps': n_steps,
                    'candle_input_type': candle_input_type,
                    'candle_output_type': candle_output_type,
                    'n_symbols': n_symbols,
                    'n_features': n_features,
                    'n_inputs': n_inputs,
                    'n_hidden_layers': n_hidden_layers,
                    'n_samples_train': n_samples_train,
                    'validation_split': validation_split,
                    'effective_n_epochs': effective_n_epochs,
                    'max_n_epochs': max_n_epochs,
                    'patience': patience,
                    'whole_set_train_loss_eval': whole_set_train_loss_eval,
                    'n_samples_test': n_samples_test,
                    'test_loss_eval': test_loss_eval,
                    'losses': losses,
                    'symbols': hist.symbols,
                    'bias': 0.0,
                    'n_samples_test_for_calc_bias': 0,
                    'model_config': model_config,
                    'history': history.history}

    return train_config
